{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvfJWl7ueY"
      },
      "source": [
        "# Lab 1\n",
        "\n",
        "\n",
        "## Part 1: Bilingual dictionary induction and unsupervised embedding-based MT (30%)\n",
        "*Note: this homework is based on materials from yandexdataschool [NLP course](https://github.com/yandexdataschool/nlp_course/). Feel free to check this awesome course if you wish to dig deeper.*\n",
        "\n",
        "*Refined by [Nikolay Karpachev](https://www.linkedin.com/in/nikolay-karpachev-b0146a104/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4rIjxa7uei"
      },
      "source": [
        "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
        "\n",
        "But even without parallel corpora this system can be good enough (hopefully), in particular for similar languages, e.g. Ukrainian and Russian. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSYq2GU7uew"
      },
      "source": [
        "### Frament of the Swadesh list for some slavic languages\n",
        "\n",
        "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
        "\n",
        "So we can see some kind of word invariance for different Slavic languages.\n",
        "\n",
        "\n",
        "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
        "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
        "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
        "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
        "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
        "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
        "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
        "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
        "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
        "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
        "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
        "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
        "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
        "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
        "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
        "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
        "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
        "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
        "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
        "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
        "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
        "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
        "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNM3_fjr7ue2"
      },
      "source": [
        "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLppwa527ue6"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGoVhRA7ufP"
      },
      "source": [
        "In this notebook we're going to use pretrained word vectors - FastText (original paper - https://arxiv.org/abs/1607.04606).\n",
        "\n",
        "You can download them from the official [website](https://fasttext.cc/docs/en/crawl-vectors.html). We're going to need embeddings for Russian and Ukrainian languages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
        "!gzip -d cc.ru.300.vec.gz\n",
        "\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.uk.300.vec.gz\n",
        "!gzip -d cc.uk.300.vec.gz"
      ],
      "metadata": {
        "id": "KV2-MpR-ugq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079e816a-8450-4777-f6d1-57689f3abbf5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-13 10:45:19--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1306357571 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.ru.300.vec.gz’\n",
            "\n",
            "cc.ru.300.vec.gz    100%[===================>]   1.22G  47.3MB/s    in 32s     \n",
            "\n",
            "2022-04-13 10:45:52 (38.7 MB/s) - ‘cc.ru.300.vec.gz’ saved [1306357571/1306357571]\n",
            "\n",
            "--2022-04-13 10:46:31--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.uk.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1257595219 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.uk.300.vec.gz’\n",
            "\n",
            "cc.uk.300.vec.gz    100%[===================>]   1.17G  39.2MB/s    in 32s     \n",
            "\n",
            "2022-04-13 10:47:03 (37.6 MB/s) - ‘cc.uk.300.vec.gz’ saved [1257595219/1257595219]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading and extracting the vectors, we should be able to load them using the [gensim](https://radimrehurek.com/gensim/) library:"
      ],
      "metadata": {
        "id": "Kwg26PKLv88U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u1JjQv_97ufT"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")\n",
        "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've loaded the vectors, you can use the `KeyedVectors` interface to get word embeddings and/or query most similar words by embedding:"
      ],
      "metadata": {
        "id": "Sqb_XJhkMyHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nTkXfT0W7ufk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9805d73-cf31-4061-dd22-235eb32f4c6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300,), array([ 0.0033, -0.0322, -0.0519, -0.0808, -0.0131], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "august_embedding = ru_emb[\"август\"]\n",
        "august_embedding.shape, august_embedding[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ru_emb.most_similar([august_embedding])"
      ],
      "metadata": {
        "id": "oQ2kCq-7NQPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2911d489-894e-493a-9177-8a080c9a8bd9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('август', 1.0),\n",
              " ('июль', 0.9383153915405273),\n",
              " ('сентябрь', 0.9240028858184814),\n",
              " ('июнь', 0.9222575426101685),\n",
              " ('октябрь', 0.9095538854598999),\n",
              " ('ноябрь', 0.8930036425590515),\n",
              " ('апрель', 0.8729087114334106),\n",
              " ('декабрь', 0.8652557730674744),\n",
              " ('март', 0.8545796275138855),\n",
              " ('февраль', 0.8401416540145874)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The latter function also allows you to vary the amount of closest words via the `topn` argument:"
      ],
      "metadata": {
        "id": "t5EcMMI6pxzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ru_emb.most_similar([august_embedding], topn=3)"
      ],
      "metadata": {
        "id": "bi6AF3z0p9Oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d998b365-58c3-4750-a43c-0f77c6009ed4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('август', 1.0),\n",
              " ('июль', 0.9383153915405273),\n",
              " ('сентябрь', 0.9240028858184814)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another feature of `KeyedVectors` is that it allows to compute embeddings for multiple words simultaneously:"
      ],
      "metadata": {
        "id": "xw345NRXov4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ru_emb[[\"август\", \"сентябрь\"]].shape"
      ],
      "metadata": {
        "id": "86OuYeLYow0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9afb90-b3ff-4f40-b708-6d19e6218c1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything above is true for the embeddings for Ukrainian language."
      ],
      "metadata": {
        "id": "3uGx5zHXQtfo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vdBA8lcg7ufs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecc8f7e-0f33-4ee6-eec1-7869a86b80ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('серпень', 0.9999999403953552),\n",
              " ('липень', 0.9096440076828003),\n",
              " ('вересень', 0.901697039604187),\n",
              " ('червень', 0.8992519378662109),\n",
              " ('жовтень', 0.8810408711433411),\n",
              " ('листопад', 0.8787633776664734),\n",
              " ('квітень', 0.8592804670333862),\n",
              " ('грудень', 0.8586863279342651),\n",
              " ('травень', 0.8408110737800598),\n",
              " ('лютий', 0.8256431818008423)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "uk_emb.most_similar([uk_emb[\"серпень\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, russian and ukrainian embeddings were trained independently of each other. This means, that there is no obvious connection between values in embeddings for similar words in Russian and Ukrainian:"
      ],
      "metadata": {
        "id": "F1Dkka5uQ37-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_yJvcKXO7uf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811ba3fc-72e4-4a2c-fd69-0467bc8f7513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stepashka.com', 0.2757962942123413),\n",
              " ('ЖИЗНИВадим', 0.25203436613082886),\n",
              " ('2Дмитрий', 0.25048112869262695),\n",
              " ('2012Дмитрий', 0.24829231202602386),\n",
              " ('Ведущий-Алексей', 0.2443869560956955),\n",
              " ('Недопустимость', 0.24435284733772278),\n",
              " ('2Михаил', 0.23981399834156036),\n",
              " ('лексей', 0.23740756511688232),\n",
              " ('комплексн', 0.23695150017738342),\n",
              " ('персональ', 0.2368222028017044)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ru_emb.most_similar([uk_emb[\"серпень\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation"
      ],
      "metadata": {
        "id": "Lia_h7W2qL8C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdYAR1q7uf6"
      },
      "source": [
        "We'll build a simple translator, which will try to predict the russian embedding from the ukrainian one. For this we'll need a dataset of word pairs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "train_data_url = \"https://raw.githubusercontent.com/girafe-ai/natural-language-processing/22s_made/homeworks/lab01_unsupervised_translation/uk_ru.train.tsv\"\n",
        "train_data = pd.read_csv(train_data_url, sep=\"\\t\", header=None)\n",
        "train_data.columns = [\"uk\", \"ru\"]\n",
        "print(f\"Train dataset size: {len(train_data)}\")\n",
        "\n",
        "test_data_url = \"https://raw.githubusercontent.com/girafe-ai/natural-language-processing/22s_made/homeworks/lab01_unsupervised_translation/uk_ru.test.tsv\"\n",
        "test_data = pd.read_csv(test_data_url, sep=\"\\t\", header=None)\n",
        "test_data.columns = [\"uk\", \"ru\"]\n",
        "print(f\"Test dataset size: {len(test_data)}\")\n",
        "\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "Kon7ZH6wUYdN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "86238a65-6ef6-4561-cbf1-ede158c040c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1927\n",
            "Test dataset size: 400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         uk          ru\n",
              "0  iмовірно    вероятно\n",
              "1     iснує  существует\n",
              "2     iспит     экзамен\n",
              "3     абияк  как-нибудь\n",
              "4       або         или"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92e22454-39eb-4c34-aeb0-36729de0d5e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uk</th>\n",
              "      <th>ru</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>iмовірно</td>\n",
              "      <td>вероятно</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>iснує</td>\n",
              "      <td>существует</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>iспит</td>\n",
              "      <td>экзамен</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>абияк</td>\n",
              "      <td>как-нибудь</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>або</td>\n",
              "      <td>или</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92e22454-39eb-4c34-aeb0-36729de0d5e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92e22454-39eb-4c34-aeb0-36729de0d5e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92e22454-39eb-4c34-aeb0-36729de0d5e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our method won't work with unknown words, so let's filter them out:"
      ],
      "metadata": {
        "id": "DYoXmFPanrwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for _, row in train_data.iterrows():\n",
        "    if row[\"uk\"] not in uk_emb or row[\"ru\"] not in ru_emb:\n",
        "        continue\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "train_data = pd.DataFrame(rows)\n",
        "print(f\"Train dataset size: {len(train_data)}\")\n",
        "\n",
        "rows = []\n",
        "for _, row in test_data.iterrows():\n",
        "    if row[\"uk\"] not in uk_emb or row[\"ru\"] not in ru_emb:\n",
        "        continue\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "test_data = pd.DataFrame(rows)\n",
        "print(f\"Test dataset size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "Ls4h2PrplJID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68c4d85-ef5d-4943-b853-094972d768d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1880\n",
            "Test dataset size: 393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our model to predict embedding for the russian word from embedding of its ukrainian counterpart. For this reason we split our train and test data into ukrainian and russian words and compute corresponding embeddings to obtain `X` (ukrainian embeddings) and `y` (russian embeddings)."
      ],
      "metadata": {
        "id": "wwjYGFE7Ui0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = uk_emb[train_data[\"uk\"].values], ru_emb[train_data[\"ru\"].values]\n",
        "X_test, Y_test = uk_emb[test_data[\"uk\"].values], ru_emb[test_data[\"ru\"].values]"
      ],
      "metadata": {
        "id": "WR7v7lvFYWYy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBBNvpz7ugQ"
      },
      "source": [
        "## Embedding space mapping (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Dhk5gL7ugS"
      },
      "source": [
        "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called Procrustes problem:\n",
        "\n",
        "$$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$W^*= \\arg\\min_W \\|XW^T - Y\\|_F$$\n",
        "\n",
        "where $\\|\\cdot\\|_F$ denotes Frobenius norm.\n",
        "\n",
        "> **Note:** in second formula, $W$ and $x$ seem to have switched places. This happens because the $X$ matrix is composed of objects $x_i$ in *rows* not *columns*, i.e. it is kind of composed of $x_i^T$. This means that $X \\in \\mathbb{R}^{N \\times D}$, where $N$ is the number of items and $D$ is the embedding dimensionality. The same is true for the $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acOjDdtL7ugY"
      },
      "source": [
        "$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$ looks like simple multiple linear regression without bias. The `sklearn` allows you to turn off the bias in `LinearRegression` via the `fit_intercept` argument (in fact they simply call bias the intercept). So let's code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lb-KN1be7uga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e313d4b-ed9f-4cd3-b251-10c8ddda918b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(fit_intercept=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "mapping = LinearRegression(fit_intercept = False)\n",
        "mapping.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tqJwoY7ugf"
      },
      "source": [
        "Let's take a look at neigbours of the vector of word _\"серпень\"_ (_\"август\"_ in Russian) after linear transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "31SrFSbn7ugi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b594e5df-b504-47ba-e2ec-5b9df68f0dae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('апрель', 0.8531432747840881),\n",
              " ('июнь', 0.8402522802352905),\n",
              " ('март', 0.8385884165763855),\n",
              " ('сентябрь', 0.8331484794616699),\n",
              " ('февраль', 0.8311208486557007),\n",
              " ('октябрь', 0.8278019428253174),\n",
              " ('ноябрь', 0.8243728280067444),\n",
              " ('июль', 0.8229618072509766),\n",
              " ('август', 0.8112280368804932),\n",
              " ('январь', 0.8022986650466919)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
        "ru_emb.most_similar(august)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okSkjk597ugo"
      },
      "source": [
        "We can see that neighbourhood of this embedding cosists of different months, but right variant is on the ninth place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uY6Y9B7ugt"
      },
      "source": [
        "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed ukrainian embedding we count how many right target pairs are found in top N nearest neighbours in russian embedding space)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zptuho8LAfIE"
      },
      "outputs": [],
      "source": [
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    total = len(pairs)\n",
        "    correct = 0\n",
        "    for i in range(total):\n",
        "        pair = pairs[i]\n",
        "        predicted_vector = mapped_vectors[i]\n",
        "        # print(predicted_vector)\n",
        "        nn = ru_emb.most_similar([predicted_vector])[:topn]\n",
        "        for n in nn:\n",
        "          if n[0] == pair[1]:\n",
        "            correct +=1\n",
        "        # YOUR CODE HERE\n",
        "      \n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "duhj9hpv7ugy"
      },
      "outputs": [],
      "source": [
        "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that our `precision` function accepts lists of pairs of words, whereas we have dataframes. However, it is not a problem: we can get a list (actually, numpy array) of pairs via the `values` property."
      ],
      "metadata": {
        "id": "z5A9tWtnuFx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0-iyd5gP7ug5"
      },
      "outputs": [],
      "source": [
        "assert precision(test_data.values, X_test) == 0.0\n",
        "assert precision(test_data.values, Y_test) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how well our model is doing."
      ],
      "metadata": {
        "id": "7DVV5lqrua_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U-ssEJ3x7uhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf604a1-d621-47a6-ce42-1c22e0e07e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 precision 62.8%\n",
            "Top-5 precision 79.1%\n"
          ]
        }
      ],
      "source": [
        "top1 = precision(test_data.values, mapping.predict(X_test), 1)\n",
        "print(f\"Top-1 precision {100 * top1:.1f}%\")\n",
        "\n",
        "top5 = precision(test_data.values, mapping.predict(X_test), 5)\n",
        "print(f\"Top-5 precision {100 * top5:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6Ou8bx7uhH"
      },
      "source": [
        "## Making it better (orthogonal Procrustean problem) (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oLs-drN7uhK"
      },
      "source": [
        "It can be shown that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
        "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
        "\n",
        "$$(W^T)^*= \\arg\\min_{W^T} \\|XW^T - Y\\|_F \\text{, where: } W^TW = I$$\n",
        "\n",
        "$$I \\text{- identity matrix}$$\n",
        "\n",
        "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "$$(W^T)^*=UV^T$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DdFQ7qti7uhL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Compute the orthogonal mapping (W^T)^* as defined in formula above.\n",
        "u, s, vh = np.linalg.svd(X_train.T @ Y_train)\n",
        "mapping = u @ vh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our `mapping` is just a numpy array, meaning that it has no `predict` method. However, from the formulae above we know, that prediction is done using the matrix multiplication:"
      ],
      "metadata": {
        "id": "sehLFmlBysc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OVOFYYa37uhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73638a33-bcc1-473a-8435-b90ce32f1b80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('апрель', 0.8245131969451904),\n",
              " ('июнь', 0.805662989616394),\n",
              " ('сентябрь', 0.8055761456489563),\n",
              " ('март', 0.8032935261726379),\n",
              " ('октябрь', 0.7987102270126343),\n",
              " ('июль', 0.7946797013282776),\n",
              " ('ноябрь', 0.7939636707305908),\n",
              " ('август', 0.7938189506530762),\n",
              " ('февраль', 0.7923861145973206),\n",
              " ('декабрь', 0.7715375423431396)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "august = uk_emb[\"серпень\"] @ mapping\n",
        "ru_emb.most_similar([august])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute our precision values and see, whether our trick did improve the results."
      ],
      "metadata": {
        "id": "h4qKCmq7zJDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r297sYP37uhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4300ee16-4897-4f5a-a5e0-2f56d22332ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 precision 64.4%\n",
            "Top-5 precision 79.9%\n"
          ]
        }
      ],
      "source": [
        "top1 = precision(test_data.values, X_test @ mapping, 1)\n",
        "print(f\"Top-1 precision {100 * top1:.1f}%\")\n",
        "\n",
        "top5 = precision(test_data.values, X_test @ mapping, 5)\n",
        "print(f\"Top-5 precision {100 * top5:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUZ72U5AfJg"
      },
      "source": [
        "## Unsupervised embedding-based MT (0.4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLyuVfHBLrJn"
      },
      "source": [
        "Now, let's build our word embeddings-based translator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPAURW1CMuP7"
      },
      "source": [
        "Firstly, download OPUS Tatoeba corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F80kUKzQMsDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98275e1-0034-466b-be26-4185c65ad8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-13 11:06:47--  https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/mono/uk.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1819128 (1.7M) [application/gzip]\n",
            "Saving to: ‘uk.txt.gz’\n",
            "\n",
            "uk.txt.gz           100%[===================>]   1.73M  2.12MB/s    in 0.8s    \n",
            "\n",
            "2022-04-13 11:06:48 (2.12 MB/s) - ‘uk.txt.gz’ saved [1819128/1819128]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/mono/uk.txt.gz\n",
        "!gzip -d ./uk.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2MV3VvoVUX5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc807255-ef1d-4d15-80f4-a4e963b79437"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Я вже закінчу коледж, коли ви вернетеся з Америки.\\n',\n",
              " 'Він наказав мені негайно вийти з кімнати.\\n',\n",
              " 'Як би ти не намагався, ти не вивчиш англійську за два-три місяці.\\n',\n",
              " 'Поки я не подзвонив, він не прийшов.\\n',\n",
              " 'У всесвіті багато галактик.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "with open('./uk.txt', 'r') as f:\n",
        "    uk_corpus = f.readlines()\n",
        "\n",
        "# To save your time and CPU, feel free to use first 1000 sentences of the corpus\n",
        "uk_corpus = uk_corpus[:1000]\n",
        "uk_corpus[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's translate these sentences word-by-word. Before that, however, don't forget to tokenize your sentences. For that you may (or may not) find the `nltk.tokenize.WordPunctTokenizer` to be very useful."
      ],
      "metadata": {
        "id": "oa3dAZHv1wjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "YDhU9O7f56GM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FGksC7l_NMi9"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in Ukrainian (str)\n",
        "    :returns:\n",
        "        translation - sentence in Russian (str)\n",
        "\n",
        "    * find ukrainian embedding for each word in sentence\n",
        "    * transform ukrainian embedding vector\n",
        "    * find nearest russian word and replace\n",
        "    \"\"\"\n",
        "    \n",
        "    translated = []\n",
        "    tk = WordPunctTokenizer()\n",
        "    ukr_tokens = tk.tokenize(sentence)\n",
        "    for token in ukr_tokens:\n",
        "      if token in uk_emb:\n",
        "        word = ru_emb.most_similar([uk_emb[token] @ mapping])[0][0]\n",
        "        translated.append(word)\n",
        "      else:\n",
        "        translated.append(token+\"(unk)\")\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return \" \".join(translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4hbbMy-tNxlf"
      },
      "outputs": [],
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"1 , 3\") == \"1 , 3\"\n",
        "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6I2ce7O_HI"
      },
      "source": [
        "Now you can play with your model and try to get as accurate translations as possible. **Note**: one big issue is out-of-vocabulary words. Try to think of various ways of handling it (you can start with translating each of them to a special **UNK** token and then move to more sophisticated approaches). Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ap1W7ZCeOAVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd93344-8de8-4d73-9de9-6a8b21b97dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ОРИГИНАЛ: Я вже закінчу коледж, коли ви вернетеся з Америки.\n",
            "\n",
            "ПЕРЕВОД: Я уже закончу колледж , когда мы прибежишь со Америки .\n",
            "--------\n",
            "ОРИГИНАЛ: Місто бомбардували ворожі літаки.\n",
            "\n",
            "ПЕРЕВОД: Город бомбили враждебные самолеты .\n",
            "--------\n",
            "ОРИГИНАЛ: Можливо, я антисоціальний, але це не означає, що я не спілкуюся з людьми.\n",
            "\n",
            "ПЕРЕВОД: Возможно , мной антисоциальный , конечно это не означает , что мной не общаюсь со людьми .\n",
            "--------\n",
            "ОРИГИНАЛ: Цього ранку випала роса.\n",
            "\n",
            "ПЕРЕВОД: Впрочем утра выпала роса .\n",
            "--------\n",
            "ОРИГИНАЛ: Біда не приходить одна.\n",
            "\n",
            "ПЕРЕВОД: Беда не приходит одна .\n",
            "--------\n",
            "ОРИГИНАЛ: Подивися на той дим.\n",
            "\n",
            "ПЕРЕВОД: Посмотри по тот дым .\n",
            "--------\n",
            "ОРИГИНАЛ: Я замовив два гамбургера.\n",
            "\n",
            "ПЕРЕВОД: Я заказал два гамбургера .\n",
            "--------\n",
            "ОРИГИНАЛ: Я не хотів нікого образити.\n",
            "\n",
            "ПЕРЕВОД: Я не хотел никого обидеть .\n",
            "--------\n",
            "ОРИГИНАЛ: Гора вкрита снігом.\n",
            "\n",
            "ПЕРЕВОД: Гора покрыта снегом .\n",
            "--------\n",
            "ОРИГИНАЛ: На фотографії в дівчини корона не з золота, а з квітів.\n",
            "\n",
            "ПЕРЕВОД: по фотографии во девушки корона не со золота , а со цветов .\n",
            "--------\n",
            "ОРИГИНАЛ: У мене є мрія.\n",
            "\n",
            "ПЕРЕВОД: Во меня То мечта .\n",
            "--------\n",
            "ОРИГИНАЛ: Я приїхав у Японію з Китаю.\n",
            "\n",
            "ПЕРЕВОД: Я приехал во Японию со Китая .\n",
            "--------\n",
            "ОРИГИНАЛ: На півночі знаходиться Шотландія; на півдні — Англія; на заході — Уельс; і ще далі на заході — Північна Ірландія.\n",
            "\n",
            "ПЕРЕВОД: по север находится Шотландия ; по юге — Англия ; по востоке — Уэльс ; и ещe дальше по востоке — северная Ирландия .\n",
            "--------\n",
            "ОРИГИНАЛ: Його рідна країна — Німеччина.\n",
            "\n",
            "ПЕРЕВОД: Его родная страна — Германия .\n",
            "--------\n",
            "ОРИГИНАЛ: Берн — столиця Швейцарії.\n",
            "\n",
            "ПЕРЕВОД: Берн — столица Швейцарии .\n",
            "--------\n",
            "ОРИГИНАЛ: Він чекав на нього до десятої години.\n",
            "\n",
            "ПЕРЕВОД: Он ждал по него к десятой часа .\n",
            "--------\n",
            "ОРИГИНАЛ: Ти можеш взяти цю книгу даром.\n",
            "\n",
            "ПЕРЕВОД: Ты можешь взять ту книгу даром .\n",
            "--------\n",
            "ОРИГИНАЛ: Цей роман написав відомий американський письменник.\n",
            "\n",
            "ПЕРЕВОД: Такой роман сочинил известный американский писатель .\n",
            "--------\n",
            "ОРИГИНАЛ: Забронюйте, будьте ласкаві, кімнату біля міжнародного аеропорту в Торонто.\n",
            "\n",
            "ПЕРЕВОД: забронировать , будте ласковые , комнату возле международного аэропорта во Торонто .\n",
            "--------\n",
            "ОРИГИНАЛ: Він знає, що ти його кохаєш?\n",
            "\n",
            "ПЕРЕВОД: Он знает , что ты его влюбится ?\n",
            "--------\n",
            "ОРИГИНАЛ: Я знаю, що ти багатий.\n",
            "\n",
            "ПЕРЕВОД: Я знаю , что ты богатый .\n",
            "--------\n",
            "ОРИГИНАЛ: Ті, хто все забувають, щасливі.\n",
            "\n",
            "ПЕРЕВОД: Те , кто всё забывают , счастливые .\n",
            "--------\n",
            "ОРИГИНАЛ: В цій річці небезпечно плавати.\n",
            "\n",
            "ПЕРЕВОД: Во этой реке опасно плавать .\n",
            "--------\n",
            "ОРИГИНАЛ: Прийшов, побачив, переміг.\n",
            "\n",
            "ПЕРЕВОД: пришел , увидел , победил .\n",
            "--------\n",
            "ОРИГИНАЛ: Я ходжу до школи пішки.\n",
            "\n",
            "ПЕРЕВОД: Я хожу к школы пешком .\n",
            "--------\n",
            "ОРИГИНАЛ: Не твоя справа!\n",
            "\n",
            "ПЕРЕВОД: Не моя дело !\n",
            "--------\n",
            "ОРИГИНАЛ: Не забудь квиток.\n",
            "\n",
            "ПЕРЕВОД: Не забудь билет .\n",
            "--------\n",
            "ОРИГИНАЛ: Хто він?\n",
            "\n",
            "ПЕРЕВОД: Кто он ?\n",
            "--------\n",
            "ОРИГИНАЛ: Ви будете чай чи каву?\n",
            "\n",
            "ПЕРЕВОД: Вы будете чай ли кофе ?\n",
            "--------\n",
            "ОРИГИНАЛ: Він не піде на пікнік, як і я.\n",
            "\n",
            "ПЕРЕВОД: Он не пойдет по пикник , как и мной .\n",
            "--------\n",
            "ОРИГИНАЛ: Коли Ви народилися?\n",
            "\n",
            "ПЕРЕВОД: Когда Вы родились ?\n",
            "--------\n",
            "ОРИГИНАЛ: Це моя улюблена пісня.\n",
            "\n",
            "ПЕРЕВОД: Это моя любимая песня .\n",
            "--------\n",
            "ОРИГИНАЛ: Ми майже сім’я.\n",
            "\n",
            "ПЕРЕВОД: мы почти семь со мной .\n",
            "--------\n",
            "ОРИГИНАЛ: Який гарний сьогодні місяць!\n",
            "\n",
            "ПЕРЕВОД: Какой красивый сегодня месяц !\n",
            "--------\n",
            "ОРИГИНАЛ: Я проти будь-яких війн.\n",
            "\n",
            "ПЕРЕВОД: Я против любой – которых войны .\n",
            "--------\n",
            "ОРИГИНАЛ: Поверхня повітряної кулі — неевклідовий простір, тому для неї не виконуються правила евклідової геометрії.\n",
            "\n",
            "ПЕРЕВОД: поверхность воздушной шары — неевклідовий(unk) пространство , потому для неё не выполняются правила симметрической геометрии .\n",
            "--------\n",
            "ОРИГИНАЛ: Кажуть, що американці вважають кількість грошей, яку заробляє людина, мірилом його уміння.\n",
            "\n",
            "ПЕРЕВОД: Говорят , что американцы считают количество денег , какую зарабатывает женщина , мерилом его умение .\n",
            "--------\n",
            "ОРИГИНАЛ: Можна я примірю це плаття?\n",
            "\n",
            "ПЕРЕВОД: Можно мной примірю(unk) это платье ?\n",
            "--------\n",
            "ОРИГИНАЛ: Якщо буде гарна погода, ми доберемося туди завтра.\n",
            "\n",
            "ПЕРЕВОД: Если будет красивая погода , мы доберёмся туда завтра .\n",
            "--------\n",
            "ОРИГИНАЛ: Це був злий заєць.\n",
            "\n",
            "ПЕРЕВОД: Это был злой заяц .\n",
            "--------\n",
            "ОРИГИНАЛ: Один, два, три, чотири, п'ять, шість, сім, вісім, дев'ять, десять.\n",
            "\n",
            "ПЕРЕВОД: Один , два , три , четыре , аш со пять , восемь , семь , восемь , девять со пять , десять .\n",
            "--------\n",
            "ОРИГИНАЛ: Хто в любові не знається, той горя не знає.\n",
            "\n",
            "ПЕРЕВОД: Кто во любви не знает , тот горя не знает .\n",
            "--------\n",
            "ОРИГИНАЛ: Його мати хвилюється за нього.\n",
            "\n",
            "ПЕРЕВОД: Его иметь волнуется за него .\n",
            "--------\n",
            "ОРИГИНАЛ: Я поважаю тих, хто старається з усіх сил.\n",
            "\n",
            "ПЕРЕВОД: Я уважаю тех , кто старается со всех сил .\n",
            "--------\n",
            "ОРИГИНАЛ: Їхня дружба переросла у глибоке кохання.\n",
            "\n",
            "ПЕРЕВОД: необычайная дружба переросла во глубокое любовь .\n",
            "--------\n",
            "ОРИГИНАЛ: Кейт п’є багато молока кожен день.\n",
            "\n",
            "ПЕРЕВОД: Рейчел аш со То много молока каждый день .\n",
            "--------\n",
            "ОРИГИНАЛ: Він злодій.\n",
            "\n",
            "ПЕРЕВОД: Он вор .\n",
            "--------\n",
            "ОРИГИНАЛ: Шумового забруднення можна було б позбігнути тільки якщо б люди були більш чутливими до навколишнього середовища.\n",
            "\n",
            "ПЕРЕВОД: Шумового(unk) загрязнение можно было бы позбігнути(unk) только если бы люди были более чувствительны к окружающей среды .\n",
            "--------\n",
            "ОРИГИНАЛ: Чай з лимоном, будьте ласкаві.\n",
            "\n",
            "ПЕРЕВОД: чай со лимоном , будте ласковые .\n",
            "--------\n",
            "ОРИГИНАЛ: Не плутай бажання з коханням.\n",
            "\n",
            "ПЕРЕВОД: Не путать желание со влюбленностью .\n",
            "--------\n",
            "ОРИГИНАЛ: Я би з задоволенням написав сотні речень в Tatoeb’і, але в мене є справи.\n",
            "\n",
            "ПЕРЕВОД: Я бы со удовольствием сочинил сотни сложноподчинённые во Tatoeb(unk) со и , конечно во меня То дела .\n",
            "--------\n",
            "ОРИГИНАЛ: Дайте мені філіжанку кави.\n",
            "\n",
            "ПЕРЕВОД: Дайте мне чашечку кофе .\n",
            "--------\n",
            "ОРИГИНАЛ: Але ж ти ніколи мені про це не розповідала!\n",
            "\n",
            "ПЕРЕВОД: ведь же ты никогда мне о это не рассказывала !\n",
            "--------\n",
            "ОРИГИНАЛ: У тебе будуть проблеми, якщо твої батьки довідаються.\n",
            "\n",
            "ПЕРЕВОД: Во тебя будут проблемы , если твои родители узнают .\n",
            "--------\n",
            "ОРИГИНАЛ: Запах троянд наповнив кімнату.\n",
            "\n",
            "ПЕРЕВОД: Запах роз наполнил комнату .\n",
            "--------\n",
            "ОРИГИНАЛ: Як у тебе справи?\n",
            "\n",
            "ПЕРЕВОД: Как во тебя дела ?\n",
            "--------\n",
            "ОРИГИНАЛ: Це мої штани.\n",
            "\n",
            "ПЕРЕВОД: Это мои штаны .\n",
            "--------\n",
            "ОРИГИНАЛ: Ні, дякую.\n",
            "\n",
            "ПЕРЕВОД: НЕт , спасибо .\n",
            "--------\n",
            "ОРИГИНАЛ: Я не розумію, чому Німеччина перемогла на Євробаченні.\n",
            "\n",
            "ПЕРЕВОД: Я не понимаю , почему Германия победила по Евровидении .\n",
            "--------\n",
            "ОРИГИНАЛ: Добрий вечір.\n",
            "\n",
            "ПЕРЕВОД: Добрый вечер .\n",
            "--------\n",
            "ОРИГИНАЛ: З юбілеєм Олексія Дударева привітав Президент Білорусі Олександр Лукашенко.\n",
            "\n",
            "ПЕРЕВОД: Со юбілеєм(unk) Алексея Палашка поприветствовал президент Белоруссии Александр Лукашенко .\n",
            "--------\n",
            "ОРИГИНАЛ: Чумацький шлях — широкий пояс із далеких зірок, кожна зірка — сонце, таке як наше.\n",
            "\n",
            "ПЕРЕВОД: Млечный путь — широкий пояс со далеких звёзд , каждая звезда — солнце , такое как наше .\n",
            "--------\n",
            "ОРИГИНАЛ: Незвичайно бачити рок-зірок з краваткою!\n",
            "\n",
            "ПЕРЕВОД: удивительно видеть рок – звёзд со галстук !\n",
            "--------\n",
            "ОРИГИНАЛ: Усе печиво у формі зірок.\n",
            "\n",
            "ПЕРЕВОД: всё печенье во форме звёзд .\n",
            "--------\n",
            "ОРИГИНАЛ: Що мені вдягнути — штани чи спідницю?\n",
            "\n",
            "ПЕРЕВОД: ЧТо мне одеть — штаны ли юбку ?\n",
            "--------\n",
            "ОРИГИНАЛ: Гартман Вітвер — відомий львівський скульптор.\n",
            "\n",
            "ПЕРЕВОД: Краусс утверждал — известный московский скульптор .\n",
            "--------\n",
            "ОРИГИНАЛ: То був злий кролик.\n",
            "\n",
            "ПЕРЕВОД: Ой был злой кролик .\n",
            "--------\n",
            "ОРИГИНАЛ: Можеш взяти будь-який, що тобі до сподоби.\n",
            "\n",
            "ПЕРЕВОД: Можешь взять любой – который , что тебе к отвратиться .\n",
            "--------\n",
            "ОРИГИНАЛ: Звичайно я піду.\n",
            "\n",
            "ПЕРЕВОД: Конечно мной пойду .\n",
            "--------\n",
            "ОРИГИНАЛ: Шовкопряди прядуть кокони.\n",
            "\n",
            "ПЕРЕВОД: шелковичные прядут коконы .\n",
            "--------\n",
            "ОРИГИНАЛ: Що б ти зробила, якщо б у тебе було, скажім, десять тисяч доларів?\n",
            "\n",
            "ПЕРЕВОД: ЧТо бы ты сделала , если бы во тебя было , замечу , десять тысяч долларов ?\n",
            "--------\n",
            "ОРИГИНАЛ: Він думає, що він хтось, а насправді він ніхто.\n",
            "\n",
            "ПЕРЕВОД: Он думает , что он кто-то , а действительно он никто .\n",
            "--------\n",
            "ОРИГИНАЛ: Вона дуже пишається своєю колекцією марок.\n",
            "\n",
            "ПЕРЕВОД: она очень гордится своею коллекцией марок .\n",
            "--------\n",
            "ОРИГИНАЛ: Він дуже простий...\n",
            "\n",
            "ПЕРЕВОД: Он очень простой ...\n",
            "--------\n",
            "ОРИГИНАЛ: Яка ти добра!\n",
            "\n",
            "ПЕРЕВОД: Какая ты добра !\n",
            "--------\n",
            "ОРИГИНАЛ: Як я за тобою скучив!\n",
            "\n",
            "ПЕРЕВОД: Как мной за тобой соскучился !\n",
            "--------\n",
            "ОРИГИНАЛ: Це все, що я знаю.\n",
            "\n",
            "ПЕРЕВОД: Это всё , что мной знаю .\n",
            "--------\n",
            "ОРИГИНАЛ: Ти ведеш щоденник?\n",
            "\n",
            "ПЕРЕВОД: Ты ведёшь дневник ?\n",
            "--------\n",
            "ОРИГИНАЛ: Тобі вирішувати.\n",
            "\n",
            "ПЕРЕВОД: Тебе решать .\n",
            "--------\n",
            "ОРИГИНАЛ: Це пошта, а то — банк.\n",
            "\n",
            "ПЕРЕВОД: Это почта , а то — банк .\n",
            "--------\n",
            "ОРИГИНАЛ: Це все, що я хочу зробити.\n",
            "\n",
            "ПЕРЕВОД: Это всё , что мной хочу сделать .\n",
            "--------\n",
            "ОРИГИНАЛ: Я вперше дивлюся такий страшний фільм.\n",
            "\n",
            "ПЕРЕВОД: Я впервые смотрю такой страшный фильм .\n",
            "--------\n",
            "ОРИГИНАЛ: Ця пісня нагадує мені про дім.\n",
            "\n",
            "ПЕРЕВОД: Этa песня напоминает мне о дом .\n",
            "--------\n",
            "ОРИГИНАЛ: Хіросі тут?\n",
            "\n",
            "ПЕРЕВОД: Хироси здесь ?\n",
            "--------\n",
            "ОРИГИНАЛ: Мене звуть Джек.\n",
            "\n",
            "ПЕРЕВОД: Меня зовут Эдди .\n",
            "--------\n",
            "ОРИГИНАЛ: Як людина живе, так вона і помре.\n",
            "\n",
            "ПЕРЕВОД: Как женщина живет , так она и умрет .\n",
            "--------\n",
            "ОРИГИНАЛ: Я тут уже дві години.\n",
            "\n",
            "ПЕРЕВОД: Я здесь уже две часа .\n",
            "--------\n",
            "ОРИГИНАЛ: Мені треба вибачитись перед Ен.\n",
            "\n",
            "ПЕРЕВОД: Мне надо извиниться перед Нб .\n",
            "--------\n",
            "ОРИГИНАЛ: Сьогодні я бачив шпака.\n",
            "\n",
            "ПЕРЕВОД: Сегодня мной видел скворца .\n",
            "--------\n",
            "ОРИГИНАЛ: «Скільки коштує ця носова хусточка?» — «Дев'яносто п'ять центів».\n",
            "\n",
            "ПЕРЕВОД: « Сколько стоить та носовая косыночка ?»(unk) — « тринадцать со двадцать аш со пять центов ».(unk)\n",
            "--------\n",
            "ОРИГИНАЛ: Ранені ведмеді, як правило, дуже небезпечні.\n",
            "\n",
            "ПЕРЕВОД: солдаты медведи , как правило , очень опасные .\n",
            "--------\n",
            "ОРИГИНАЛ: Він швидко втомлюється.\n",
            "\n",
            "ПЕРЕВОД: Он быстро устает .\n",
            "--------\n",
            "ОРИГИНАЛ: Усі готові.\n",
            "\n",
            "ПЕРЕВОД: остальные готовы .\n",
            "--------\n",
            "ОРИГИНАЛ: Він скучає по своїй сім'ї.\n",
            "\n",
            "ПЕРЕВОД: Он скучает по своей семь со мье .\n",
            "--------\n",
            "ОРИГИНАЛ: «Дякую», — «На здоров'я».\n",
            "\n",
            "ПЕРЕВОД: « Спасибо »,(unk) — « по здоровье со мной ».(unk)\n",
            "--------\n",
            "ОРИГИНАЛ: Я ще не знаю своєї адреси, я певний час буду жити в подруги.\n",
            "\n",
            "ПЕРЕВОД: Я ещe не знаю своего адреса , мной определенный момент буду жить во подруги .\n",
            "--------\n",
            "ОРИГИНАЛ: Амазонка— друга по довжині ріка в світі після Ніла.\n",
            "\n",
            "ПЕРЕВОД: Амазонка — вторая по длине река во мире после Нила .\n",
            "--------\n",
            "ОРИГИНАЛ: А якщо побачиш Тома, передай йому від мене вітання.\n",
            "\n",
            "ПЕРЕВОД: А если увидишь Тима , передай ему от меня поздравления .\n",
            "--------\n",
            "ОРИГИНАЛ: Закрий за собою двері.\n",
            "\n",
            "ПЕРЕВОД: закрой за собой дверь .\n",
            "--------\n",
            "ОРИГИНАЛ: Тримай при собі словник.\n",
            "\n",
            "ПЕРЕВОД: Держи при себе словарь .\n",
            "--------\n"
          ]
        }
      ],
      "source": [
        "for sent in uk_corpus[::10]:\n",
        "    print(\"ОРИГИНАЛ:\", sent)\n",
        "    print(\"ПЕРЕВОД:\",translate(sent))\n",
        "    print(\"--------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMxWUtipDD8"
      },
      "source": [
        "Great! \n",
        "See second notebook for the Neural Machine Translation assignment."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Spirina Lab1_NLP_part1_Embedding_based_MT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}